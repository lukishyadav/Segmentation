{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query for cell below\n",
    "# change tenant_id and timestamp\n",
    "\n",
    "\"\"\"\n",
    "SELECT \"tenant_id\",\n",
    "         from_iso8601_timestamp(timestamp) AS \"event_datetime\",\n",
    "         \"object_data-vin\" as \"vin\",\n",
    "         \"object_data-is_available\" as \"is_available\",\n",
    "         \"object_data-last_known_latitude\" as \"lat\",\n",
    "         \"object_data-last_known_longitude\" as \"lng\"\n",
    "FROM \"data_lake_us_prod\".\"sa_object_changed\"\n",
    "WHERE \"name\" = 'VEHICLE_UPDATE'\n",
    "        AND tenant_id = 'darwin-prod'\n",
    "        AND timestamp > '2019-05-01'\n",
    "        AND timestamp < '2019-05-15'\n",
    "order by timestamp;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings\n",
    "region = {\n",
    "     'oakland': dict(\n",
    "         x_min=-13618976.4221,\n",
    "         x_max=-13605638.1607,\n",
    "         y_min=4549035.0828,\n",
    "         y_max=4564284.2700,\n",
    "         timezone='America/Los_Angeles'),\n",
    "     'madrid': dict(\n",
    "         x_min=-416448.0394,\n",
    "         x_max=-406912.5201,\n",
    "         y_min=4921025.4356,\n",
    "         y_max=4931545.0816,\n",
    "         timezone='Europe/Madrid')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3267, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-2-b0a14141cf17>\", line 113, in <module>\n",
      "    df.apply(extractor)\n",
      "  File \"/anaconda3/lib/python3.7/site-packages/pandas/core/series.py\", line 3194, in apply\n",
      "    mapped = lib.map_infer(values, f, convert=convert_dtype)\n",
      "  File \"pandas/_libs/src/inference.pyx\", line 1472, in pandas._libs.lib.map_infer\n",
      "  File \"<ipython-input-2-b0a14141cf17>\", line 82, in extractor\n",
      "    mi_df = mi_df.append(temp, ignore_index=True)\n",
      "  File \"/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\", line 6211, in append\n",
      "    sort=sort)\n",
      "  File \"/anaconda3/lib/python3.7/site-packages/pandas/core/reshape/concat.py\", line 226, in concat\n",
      "    return op.get_result()\n",
      "  File \"/anaconda3/lib/python3.7/site-packages/pandas/core/reshape/concat.py\", line 423, in get_result\n",
      "    copy=self.copy)\n",
      "  File \"/anaconda3/lib/python3.7/site-packages/pandas/core/internals.py\", line 5418, in concatenate_block_managers\n",
      "    [ju.block for ju in join_units], placement=placement)\n",
      "  File \"/anaconda3/lib/python3.7/site-packages/pandas/core/internals.py\", line 368, in concat_same_type\n",
      "    axis=self.ndim - 1)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2018, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda3/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1095, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/anaconda3/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 313, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/anaconda3/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 347, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/anaconda3/lib/python3.7/inspect.py\", line 1500, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/anaconda3/lib/python3.7/inspect.py\", line 1458, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/anaconda3/lib/python3.7/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/anaconda3/lib/python3.7/inspect.py\", line 739, in getmodule\n",
      "    f = getabsfile(module)\n",
      "  File \"/anaconda3/lib/python3.7/inspect.py\", line 708, in getabsfile\n",
      "    _filename = getsourcefile(object) or getfile(object)\n",
      "  File \"/anaconda3/lib/python3.7/inspect.py\", line 693, in getsourcefile\n",
      "    if os.path.exists(filename):\n",
      "  File \"/anaconda3/lib/python3.7/genericpath.py\", line 19, in exists\n",
      "    os.stat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "# generates dataframe of vehicle availability merged with aggregate dow/hour availability\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import calendar\n",
    "from copy import deepcopy\n",
    "\n",
    "DATAFILE = 'vehicle_data_eiffel_2019_04.csv'\n",
    "REGION_TIMEZONE = region['madrid']['timezone']\n",
    "DT_COLS = ['event_datetime']\n",
    "\n",
    "# create multi-index and multi-index dataframe\n",
    "mi = pd.MultiIndex.from_product([list(calendar.day_name), list(range(0, 24))], names=['dow', 'hour'])\n",
    "base_series = pd.Series(index=mi).fillna(value=0)\n",
    "mi_df = pd.DataFrame(columns=mi)\n",
    "\n",
    "\n",
    "def convert_datetime_columns(df, columns):\n",
    "    for col in columns:\n",
    "        df[col] = df[col].dt.tz_localize('UTC').dt.tz_convert(REGION_TIMEZONE)\n",
    "\n",
    "        \n",
    "def collapse_is_available_events(group):\n",
    "    global supply_df\n",
    "    group = group.sort_values(by='event_datetime')\n",
    "\n",
    "    # get time of change of states\n",
    "    \n",
    "    # get event_datetime when is_available goes from true to false (becomes unavailable)\n",
    "    # previous event (is_available=True) changed state (is_available=False), indicating becoming unavailable\n",
    "    left = group[(group['is_available'] == False) & (group['is_available'].shift() == True)].rename(\n",
    "        columns={'event_datetime':'unavailable_at'})\n",
    "        \n",
    "    # get event_datetime when is_available goes from false to true (becomes available)\n",
    "    # previous event (is_available=False) changed state (is_available=True), indicating becoming available\n",
    "    right = group[(group['is_available'] == True) & (group['is_available'].shift() == False)].rename(\n",
    "        columns={'event_datetime':'available_at'})['available_at'].to_frame()\n",
    "\n",
    "    # can't assume symmetry for events\n",
    "    # can't tell which event comes first\n",
    "    merged_group = pd.merge_asof(left, right, left_on='unavailable_at', right_on='available_at')\n",
    "    supply_df = supply_df.append(merged_group)\n",
    "    \n",
    "    \n",
    "# construct large dow/hour df\n",
    "# NOTE: very expensive. should save intermediates so don't have to regenerate\n",
    "def extractor(x):\n",
    "    global mi_df\n",
    "    temp = deepcopy(base_series)\n",
    "    # duration less than 1 hour, does span across slice (hour) ex: [1:30, 2:15]\n",
    "    if x.size == 2 and x[0].hour != x[1].hour:  \n",
    "        temp[x[0].day_name(), x[0].hour] += 60 - x[0].minute\n",
    "        temp[x[1].day_name(), x[1].hour] += x[1].minute\n",
    "\n",
    "    # duration less than 1 hour, doesn't span across slice (hour) ex: [1:30, 1:45]\n",
    "    elif x.size == 2 and x[0].hour == x[1].hour:\n",
    "        temp[x[0].day_name(), x[0].hour] += x[1].minute - x[0].minute\n",
    "  \n",
    "    # duration greater than 1 hour, does span across slice (hour) ex: [1:30, 2:30, 2:45]\n",
    "    elif x.size == 3 and x[1].hour == x[2].hour:\n",
    "        temp[x[0].day_name(), x[0].hour] += 60 - x[0].minute\n",
    "        temp[x[2].day_name(), x[2].hour] += x[2].minute\n",
    "  \n",
    "    else:\n",
    "        # duration greater than 2 hours, ex: [1:30, 2:30, 3:30, 3:45]\n",
    "        # or spans across multiple hours\n",
    "        n = 0\n",
    "        min_marker = x[0].minute\n",
    "        for i, j, k in zip(x.day_name(), x.hour, x.minute):\n",
    "            # each datetimeindex\n",
    "            if n == 0: # first element => 60 - 30 = 30\n",
    "                temp[i, j] += (60 - k)\n",
    "            elif n == (x.size - 1):  # last element, can't assume full hour\n",
    "                if k >= min_marker:\n",
    "                    temp[i, j] += (k - min_marker) # ex: 3:45 - 3:30 = 15m\n",
    "                else:\n",
    "                    temp[i, j] += k  # ex: 3:30 - 3:00 = 30m\n",
    "            elif n == (x.size - 2):  # second to last element, can't assume full hour\n",
    "                temp[i, j] += k  # ex: 3:30 - 3:00 = 30m\n",
    "            else:  # middle of array\n",
    "                temp[i, j] += 60 # ex: 3:30 - 2:30 = 1h\n",
    "            n += 1\n",
    "    mi_df = mi_df.append(temp, ignore_index=True)\n",
    "\n",
    "\n",
    "# get df and clean up\n",
    "vehicle_df = pd.read_csv(\n",
    "    DATAFILE,\n",
    "    parse_dates=['event_datetime'],\n",
    "    infer_datetime_format=True\n",
    ").dropna()\n",
    "\n",
    "convert_datetime_columns(vehicle_df, DT_COLS)\n",
    "\n",
    "# group by vin\n",
    "vehicle_df = vehicle_df.groupby(['vin'])\n",
    "supply_df = pd.DataFrame()\n",
    "\n",
    "vehicle_df.apply(collapse_is_available_events)\n",
    "supply_df = supply_df.dropna()\n",
    "\n",
    "# supply_df['unavailable_at'] = supply_df['event_datetime']\n",
    "# supply_df.drop(['event_datetime'], axis=1)\n",
    "\n",
    "supply_df.reset_index(inplace=True)\n",
    "supply_df['idle_duration'] = supply_df['unavailable_at'] - supply_df['available_at']  # duration for analysis\n",
    "supply_df['idle_duration_minutes'] = supply_df['idle_duration'].dt.total_seconds()/60.0\n",
    "\n",
    "# create datetimeindex of periods with the end datetime appended\n",
    "df = supply_df.apply(\n",
    "    lambda x: (pd.date_range(x['available_at'], x['unavailable_at'], freq='H', closed='left')).append(\n",
    "        pd.to_datetime([x['unavailable_at']])), axis=1)\n",
    "\n",
    "df.apply(extractor)\n",
    "\n",
    "# merge the big dow/hour mask back with vehicle_update data\n",
    "supply_df = supply_df.merge(mi_df, left_index=True, right_index=True)\n",
    "supply_df.to_csv(f'{DATAFILE.split(\".\")[0]}_with_dow_hour_mask.csv')\n",
    "supply_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1G1FX6S08J4138281\n",
      "2019-04-03 13:57:20.267000-07:00\n",
      "2019-04-03 14:55:38.815000-07:00\n",
      "1G1FX6S08J4138281\n",
      "2019-04-03 15:12:08.695000-07:00\n",
      "2019-04-04 09:08:48.304000-07:00\n",
      "JTDKDTB3XJ1606732\n",
      "2019-04-15 22:52:15.782000-07:00\n",
      "2019-04-15 22:52:37.183000-07:00\n",
      "JTDKDTB3XJ1606732\n",
      "2019-04-26 15:09:59.559000-07:00\n",
      "2019-04-29 06:13:42.903000-07:00\n"
     ]
    }
   ],
   "source": [
    "# spot checks (dependent on above)\n",
    "\n",
    "print(supply_df.iloc[0]['vin'])\n",
    "print(supply_df.iloc[0]['available_at'])\n",
    "print(supply_df.iloc[0]['unavailable_at'])\n",
    "\n",
    "print(supply_df.iloc[1]['vin'])\n",
    "print(supply_df.iloc[1]['available_at'])\n",
    "print(supply_df.iloc[1]['unavailable_at'])\n",
    "\n",
    "print(supply_df.iloc[50961]['vin'])\n",
    "print(supply_df.iloc[50961]['available_at'])\n",
    "print(supply_df.iloc[50961]['unavailable_at'])\n",
    "\n",
    "print(supply_df.iloc[50988]['vin'])\n",
    "print(supply_df.iloc[50988]['available_at'])\n",
    "print(supply_df.iloc[50988]['unavailable_at'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query for rental time, rental location, and total_to_charge information\n",
    "# used in cell below\n",
    "# change the tenant_id and timestamp\n",
    "'''\n",
    "SELECT \"object_data-rental_id\" as \"rental_id\",\n",
    "          \"object_data-customer_id\" as \"customer_id\",\n",
    "          \"object_data-rental_reserved_at\" as \"reserved_at\",\n",
    "          \"object_data-rental_booked_at\" as \"booked_at\",\n",
    "          \"object_data-rental_ended_at\" as \"ended_at\",\n",
    "          \"object_data-start_location_latitude\" as \"start_location_lat\",\n",
    "          \"object_data-start_location_longitude\" as \"start_location_lng\",\n",
    "          \"object_data-end_location_latitude\" as \"end_location_lat\",\n",
    "          \"object_data-end_location_longitude\" as \"end_location_lng\",\n",
    "          payments.total_to_charge\n",
    " FROM \"data_lake_us_prod\".\"sa_object_changed\"\n",
    " JOIN (\n",
    "   select \"object_data-rental_id\" as \"rental_id\",\n",
    "          \"object_data-total_to_charge\" as \"total_to_charge\"\n",
    "   from \"data_lake_us_prod\".\"sa_object_changed\"\n",
    "   where tenant_id = 'darwin-prod'\n",
    "   and name = 'PAYMENT_LIFECYCLE'\n",
    "   and timestamp > '2019-05-01'\n",
    "   and timestamp < '2019-05-15'\n",
    "   group by \"object_data-rental_id\", \"object_data-total_to_charge\"\n",
    "   having \"object_data-total_to_charge\" is not null) as payments on payments.rental_id = \"object_data-rental_id\"\n",
    " WHERE tenant_id = 'darwin-prod'\n",
    "         AND name = 'RENTAL_LIFECYCLE'\n",
    "         AND timestamp > '2019-05-01'\n",
    "         AND timestamp < '2019-05-15'\n",
    " GROUP BY\n",
    "     \"object_data-rental_id\",\n",
    "     \"object_data-customer_id\",\n",
    "     \"object_data-start_location_latitude\",\n",
    "     \"object_data-start_location_longitude\",\n",
    "     \"object_data-end_location_latitude\",\n",
    "     \"object_data-end_location_longitude\",\n",
    "     \"object_data-rental_reserved_at\",\n",
    "     \"object_data-rental_booked_at\",\n",
    "     \"object_data-rental_ended_at\",\n",
    "     payments.total_to_charge\n",
    " HAVING \"object_data-start_location_latitude\" is NOT null\n",
    "         AND \"object_data-start_location_longitude\" is NOT null\n",
    "         AND \"object_data-end_location_latitude\" is NOT null\n",
    "         AND \"object_data-end_location_longitude\" is NOT null\n",
    "         AND length(split(cast(\"object_data-end_location_latitude\" as varchar), '.')[2]) > 4\n",
    "         AND length(split(cast(\"object_data-end_location_longitude\" as varchar), '.')[2]) > 4\n",
    "         AND length(split(cast(\"object_data-start_location_latitude\" as varchar), '.')[2]) > 4\n",
    "         AND length(split(cast(\"object_data-start_location_longitude\" as varchar), '.')[2]) > 4\n",
    "'''\n",
    "\n",
    "\n",
    "'''\n",
    "SELECT \"object_data-rental_id\" as \"rental_id\",\n",
    "          \"object_data-customer_id\" as \"customer_id\",\n",
    "          \"object_data-rental_reserved_at\" as \"reserved_at\",\n",
    "          \"object_data-rental_booked_at\" as \"booked_at\",\n",
    "          \"object_data-rental_ended_at\" as \"ended_at\",\n",
    "          \"object_data-start_location_latitude\" as \"start_location_lat\",\n",
    "          \"object_data-start_location_longitude\" as \"start_location_lng\",\n",
    "          \"object_data-end_location_latitude\" as \"end_location_lat\",\n",
    "          \"object_data-end_location_longitude\" as \"end_location_lng\",\n",
    "          payments.total_to_charge,\n",
    "          payments.credit_amount_used\n",
    " FROM (select distinct \"object_data-rental_id\" as \"rental_id\"\n",
    "       from \"data_lake_us_prod\".\"sa_object_changed\"\n",
    "       where \"object_data-rental_reserved_at\" > '2019-05-01'\n",
    "       and \"object_data-rental_reserved_at\" < '2019-05-02'\n",
    "      ) as rentals_list\n",
    " JOIN \"data_lake_us_prod\".\"sa_object_changed\"\n",
    " ON \"object_data-rental_id\" = rentals_list.rental_id\n",
    " JOIN (select \"object_data-rental_id\" as \"rental_id\",\n",
    "          \"object_data-total_to_charge\" as \"total_to_charge\",\n",
    "          \"object_data-credit_amount_used\" as \"credit_amount_used\"\n",
    "       from \"data_lake_us_prod\".\"sa_object_changed\"\n",
    "       where tenant_id = 'darwin-prod'\n",
    "       and name = 'PAYMENT_LIFECYCLE'\n",
    "       group by \"object_data-rental_id\", \"object_data-total_to_charge\", \"object_data-credit_amount_used\"\n",
    "       having \"object_data-total_to_charge\" is not null\n",
    "      ) as payments \n",
    " ON payments.rental_id = rentals_list.rental_id\n",
    " WHERE tenant_id = 'darwin-prod'\n",
    "         AND name = 'RENTAL_LIFECYCLE'\n",
    " GROUP BY\n",
    "     \"object_data-rental_id\",\n",
    "     \"object_data-customer_id\",\n",
    "     \"object_data-start_location_latitude\",\n",
    "     \"object_data-start_location_longitude\",\n",
    "     \"object_data-end_location_latitude\",\n",
    "     \"object_data-end_location_longitude\",\n",
    "     \"object_data-rental_reserved_at\",\n",
    "     \"object_data-rental_booked_at\",\n",
    "     \"object_data-rental_ended_at\",\n",
    "     payments.total_to_charge,\n",
    "     payments.credit_amount_used\n",
    " HAVING \"object_data-start_location_latitude\" is NOT null\n",
    "         AND \"object_data-start_location_longitude\" is NOT null\n",
    "         AND \"object_data-end_location_latitude\" is NOT null\n",
    "         AND \"object_data-end_location_longitude\" is NOT null\n",
    "         AND length(split(cast(\"object_data-end_location_latitude\" as varchar), '.')[2]) > 4\n",
    "         AND length(split(cast(\"object_data-end_location_longitude\" as varchar), '.')[2]) > 4\n",
    "         AND length(split(cast(\"object_data-start_location_latitude\" as varchar), '.')[2]) > 4\n",
    "         AND length(split(cast(\"object_data-start_location_longitude\" as varchar), '.')[2]) > 4\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# formats rental datafile for data visualization\n",
    "# rental datafile must have \n",
    "\n",
    "# create df from selected start and end hours\n",
    "# bin the rental start positions\n",
    "import pandas as pd\n",
    "\n",
    "# get the rentals file\n",
    "DATAFILE = 'rental_data_darwin_2019_05_01_2019_05_15.csv'\n",
    "REGION_TIMEZONE = region['oakland']['timezone']\n",
    "DT_COLS = ['reserved_at', 'booked_at', 'ended_at']\n",
    "\n",
    "def convert_datetime_columns(df, columns):\n",
    "    for col in columns:\n",
    "        df[col] = df[col].dt.tz_localize('UTC').dt.tz_convert(REGION_TIMEZONE)\n",
    "\n",
    "# get df and clean up\n",
    "rental_df = pd.read_csv(\n",
    "    DATAFILE,\n",
    "    parse_dates=DT_COLS,\n",
    "    infer_datetime_format=True\n",
    ").dropna()\n",
    "\n",
    "convert_datetime_columns(rental_df, DT_COLS)\n",
    "\n",
    "# extract the rental start dow/hour\n",
    "rental_df['reserved_at_hour'] = rental_df['reserved_at'].dt.hour\n",
    "rental_df['reserved_at_dow'] = rental_df['reserved_at'].dt.day_name()\n",
    "\n",
    "rental_df.to_csv(f'{DATAFILE.split(\".\")[0]}_with_dow_hour_mask.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query for getting appopens data\n",
    "\"\"\"\n",
    "select distinct \n",
    "\"data-customer-id\",\n",
    "\"session_id\",\n",
    "from_iso8601_timestamp(\"metadata-recorder_received_at\") as \"event_datetime\",\n",
    "cast(from_iso8601_timestamp(\"metadata-recorder_received_at\") as date) as \"date\",\n",
    "extract(hour from(cast(from_iso8601_timestamp(\"metadata-recorder_received_at\") as time))) as \"hour\",\n",
    "extract(minute from(cast(from_iso8601_timestamp(\"metadata-recorder_received_at\") as time))) as \"minute\",\n",
    "\"data-number_vehicles_within_quarter_mile\",\n",
    "\"data-number_vehicles_within_mile\",\n",
    "\"data-user_location-lat\",\n",
    "\"data-user_location-lng\" \n",
    " \n",
    "from data_lake_eu_prod.ma_user_activity\n",
    "\n",
    "where \"metadata-recorder_received_at\" > '2019-05-10 00:00:00.000 UTC'\n",
    "and \"name\" not like '%NETWORK%'\n",
    "and \"name\" not like '%UNKNOWN%'\n",
    "and \"name\" in ('VEHICLE_AVAILABILITY')\n",
    "and (\"data-number_vehicles_within_quarter_mile\" is null or \"data-number_vehicles_within_quarter_mile\" > 0)\n",
    "order by \"session_id\", event_datetime\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nearest vehicle app data\n",
    "\"\"\"\n",
    "select\n",
    "from_iso8601_timestamp(timestamp) AS \"event_datetime\",\n",
    "\"data-distance\",\n",
    "\"data-user_location-lat\",\n",
    "\"data-user_location-lng\",\n",
    "\"data-vehicle_latitude\",\n",
    "\"data-vehicle_longitude\"\n",
    "from data_lake_us_prod.ma_user_activity\n",
    "where tenant_id = 'darwin-prod'\n",
    "and name = 'DISTANCE_NEAREST_VEHICLE'\n",
    "and timestamp > '2019-05-01'\n",
    "and timestamp < '2019-05-15';\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# constants\n",
    "miles_per_meter = 0.000621371\n",
    "rental_distance_threshold = 0.25\n",
    "\n",
    "def convert_datetime_columns(df, columns):\n",
    "    for col in columns:\n",
    "        df[col] = df[col].dt.tz_localize('UTC').dt.tz_convert(REGION_TIMEZONE)\n",
    "\n",
    "# load the file\n",
    "DT_COLS = ['event_datetime']\n",
    "datafile = 'nearest_vehicle_app_data_darwin_2019_05_01_2019_05_15.csv'\n",
    "df = pd.read_csv(\n",
    "        datafile,\n",
    "        parse_dates=DT_COLS,\n",
    "        infer_datetime_format=True\n",
    "    ).dropna()\n",
    "\n",
    "convert_datetime_columns(df, DT_COLS)\n",
    "\n",
    "# create new column to test if within threshold\n",
    "df['vehicle_nearby'] = (df['data-distance'] * miles_per_meter) < rental_distance_threshold\n",
    "\n",
    "df['event_hour'] = df['event_datetime'].dt.hour\n",
    "df['event_dow'] = df['event_datetime'].dt.day_name()\n",
    "\n",
    "df.to_csv(f'{datafile.split(\".\")[0]}_with_threshold_mask.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
