{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Queries Quick Reference\n",
    "\n",
    "You can generate the files using the queries listed in each analysis section, under the \"Vehicle Events Analysis\", \"App Events Analysis\", \"Rental Event Analysis\" sections. For more details, go to each individual section.\n",
    "\n",
    "For reference:\n",
    "\n",
    "- Vehicle Events Analysis Query (used for `VEHICLE_DATAFILE`)\n",
    "```sql\n",
    "SELECT from_iso8601_timestamp(timestamp) AS \"event_datetime\",\n",
    "         \"object_data-vin\" as \"vin\",\n",
    "         \"object_data-is_available\" as \"is_available\",\n",
    "         \"object_data-last_known_latitude\" as \"lat\",\n",
    "         \"object_data-last_known_longitude\" as \"lng\"\n",
    "FROM \"data_lake_us_prod\".\"sa_object_changed\" -- region\n",
    "WHERE \"name\" = 'VEHICLE_UPDATE'\n",
    "        AND tenant_id = 'darwin-prod' -- customer\n",
    "        AND timestamp > '2019-05-01' -- start date\n",
    "        AND timestamp < '2019-05-15' -- end date\n",
    "order by timestamp\n",
    "```\n",
    "\n",
    "- App Events Analysis (used for `APP_DATA_DATAFILE`)\n",
    "```sql\n",
    "select\n",
    "from_iso8601_timestamp(timestamp) AS \"event_datetime\",\n",
    "\"data-distance\",\n",
    "\"data-user_location-lat\",\n",
    "\"data-user_location-lng\",\n",
    "\"data-vehicle_latitude\",\n",
    "\"data-vehicle_longitude\"\n",
    "from data_lake_us_prod.ma_user_activity -- region\n",
    "where tenant_id = 'darwin-prod' -- customer\n",
    "and name = 'DISTANCE_NEAREST_VEHICLE'\n",
    "and timestamp > '2019-05-01' -- start date\n",
    "and timestamp < '2019-05-15' -- end date\n",
    "```\n",
    "\n",
    "- Rental Events Analysis (used for `RENTAL_DATAFILE`)\n",
    "```sql\n",
    "SELECT \"object_data-rental_id\" as \"rental_id\",\n",
    "        \"object_data-customer_id\" as \"customer_id\",\n",
    "        \"object_data-rental_reserved_at\" as \"reserved_at\",\n",
    "        \"object_data-rental_booked_at\" as \"booked_at\",\n",
    "        \"object_data-rental_ended_at\" as \"ended_at\",\n",
    "        \"object_data-start_location_latitude\" as \"start_location_lat\",\n",
    "        \"object_data-start_location_longitude\" as \"start_location_lng\",\n",
    "        \"object_data-end_location_latitude\" as \"end_location_lat\",\n",
    "        \"object_data-end_location_longitude\" as \"end_location_lng\",\n",
    "        payments.total_to_charge,\n",
    "        payments.credit_amount_used\n",
    "FROM (select distinct \"object_data-rental_id\" as \"rental_id\"\n",
    "     from \"data_lake_us_prod\".\"sa_object_changed\" -- environment\n",
    "     where \"object_data-rental_reserved_at\" > '2019-05-01' -- start date\n",
    "     and \"object_data-rental_reserved_at\" < '2019-05-15' -- end date\n",
    "    ) as rentals_list\n",
    "JOIN \"data_lake_us_prod\".\"sa_object_changed\" -- environment\n",
    "ON \"object_data-rental_id\" = rentals_list.rental_id\n",
    "JOIN (select \"object_data-rental_id\" as \"rental_id\",\n",
    "        \"object_data-total_to_charge\" as \"total_to_charge\",\n",
    "        \"object_data-credit_amount_used\" as \"credit_amount_used\"\n",
    "     from \"data_lake_us_prod\".\"sa_object_changed\" -- environment\n",
    "     where tenant_id = 'darwin-prod' -- customer\n",
    "     and name = 'PAYMENT_LIFECYCLE'\n",
    "     group by \"object_data-rental_id\", \"object_data-total_to_charge\", \"object_data-credit_amount_used\"\n",
    "     having \"object_data-total_to_charge\" is not null\n",
    "    ) as payments \n",
    "ON payments.rental_id = rentals_list.rental_id\n",
    "WHERE tenant_id = 'darwin-prod' -- customer\n",
    "       AND name = 'RENTAL_LIFECYCLE'\n",
    "       AND \"object_data-rental_reserved_at\" > '2019-05-01' -- start date\n",
    "       AND \"object_data-rental_reserved_at\" < '2019-05-15' -- end date\n",
    "GROUP BY\n",
    "   \"object_data-rental_id\",\n",
    "   \"object_data-customer_id\",\n",
    "   \"object_data-start_location_latitude\",\n",
    "   \"object_data-start_location_longitude\",\n",
    "   \"object_data-end_location_latitude\",\n",
    "   \"object_data-end_location_longitude\",\n",
    "   \"object_data-rental_reserved_at\",\n",
    "   \"object_data-rental_booked_at\",\n",
    "   \"object_data-rental_ended_at\",\n",
    "   payments.total_to_charge,\n",
    "   payments.credit_amount_used\n",
    "HAVING \"object_data-start_location_latitude\" is NOT null\n",
    "       AND \"object_data-start_location_longitude\" is NOT null\n",
    "       AND \"object_data-end_location_latitude\" is NOT null\n",
    "       AND \"object_data-end_location_longitude\" is NOT null\n",
    "       AND length(split(cast(\"object_data-end_location_latitude\" as varchar), '.')[2]) > 4\n",
    "       AND length(split(cast(\"object_data-end_location_longitude\" as varchar), '.')[2]) > 4\n",
    "       AND length(split(cast(\"object_data-start_location_latitude\" as varchar), '.')[2]) > 4\n",
    "       AND length(split(cast(\"object_data-start_location_longitude\" as varchar), '.')[2]) > 4\n",
    "```\n",
    "\n",
    "- Alternative Rental Events Analysis (from prod db)\n",
    "```sql\n",
    "select \n",
    "    r.id as rental_id,\n",
    "    r.customer_id as customer_id,\n",
    "    r.start_datetime as reserved_at,\n",
    "    r.booked_at as booked_at,\n",
    "    r.end_datetime as ended_at,\n",
    "    loc.lat as start_location_lat,\n",
    "    loc.lng as start_location_lng,\n",
    "    loc.postal_code,\n",
    "    p.total_to_charge as total_to_charge,\n",
    "    p.credit_used_amount as credit_amount_used\n",
    "from carsharing_rental as r\n",
    "    join common_location as loc on r.start_location_id = loc.id\n",
    "    join carsharing_payment on carsharing_payment.rental_id = r.id\n",
    "    join common_payment as p on p.id = carsharing_payment.payment_ptr_id\n",
    "    join common_customer on common_customer.id = r.customer_id\n",
    "    join common_userprofile on common_customer.user_profile_id = common_userprofile.id\n",
    "    join auth_user on common_userprofile.user_id = auth_user.id\n",
    "where auth_user.email not in (select email from common_paymentexemption)\n",
    "    and auth_user.email not like '%ridecell.com'\n",
    "    and auth_user.email not like '%eco-service.us'\n",
    "    and auth_user.email not like '%ecoservice.us'\n",
    "    and r.start_datetime > '2019-05-01'\n",
    "    and r.start_datetime < '2019-06-06'\n",
    "    and r.cancelled_at is null;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declare Files Below\n",
    "\n",
    "These files will be retrieved by the notebook for analysis and processing prior to visualization.\n",
    "After processing, new files will be generated with the naming convention \"{filename}\\_with\\_dow\\_hour\\_mask.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vehicle Events Analysis\n",
    "\n",
    "The following query is to be used to retrieve data from Athena. \n",
    "The query collects vehicle all vehicle events between a given time for a given customer \n",
    "and retains the \"is_available\" field to determine vehicle availability (or idleness).\n",
    "\n",
    "Change the following to suit your needs:\n",
    "1. customer: code name for customer followed by environment with this format: '{customer}-{env}' example: 'darwin-prod'\n",
    "2. start date: yyyy-mm-dd, inclusive. example: '2019-05-01'\n",
    "3. end date: yyyy-mm-dd, exclusive. example: '2019-05-15'\n",
    "4. region: 'data_lake_{region}_prod' where environment represents the region from where this data is pulled. Currently, we only support 'us' and 'eu'.\n",
    "            \n",
    "Query: \n",
    "```sql\n",
    "SELECT from_iso8601_timestamp(timestamp) AS \"event_datetime\",\n",
    "         \"object_data-vin\" as \"vin\",\n",
    "         \"object_data-is_available\" as \"is_available\",\n",
    "         \"object_data-last_known_latitude\" as \"lat\",\n",
    "         \"object_data-last_known_longitude\" as \"lng\"\n",
    "FROM \"data_lake_us_prod\".\"sa_object_changed\" -- region\n",
    "WHERE \"name\" = 'VEHICLE_UPDATE'\n",
    "        AND tenant_id = 'darwin-prod' -- customer\n",
    "        AND timestamp > '2019-05-01' -- start date\n",
    "        AND timestamp < '2019-05-15' -- end date\n",
    "order by timestamp;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import calendar\n",
    "from copy import deepcopy\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from settings import region\n",
    "\n",
    "VEHICLE_DATAFILE = 'vehicle_availability_data_eiffel_20190601_20190801.csv'\n",
    "miles_per_meter = 0.000621371"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mi = pd.MultiIndex.from_product([list(calendar.day_name), list(range(0, 24))], names=['dow', 'hour'])\n",
    "base_series = pd.Series(index=mi).fillna(value=0)\n",
    "mi_df = pd.DataFrame(columns=mi)\n",
    "supply_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert timestamp to local region timezone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION_TIMEZONE = region['oakland']['timezone']\n",
    "\n",
    "def convert_datetime_columns(df, columns):\n",
    "    for col in columns:\n",
    "        try:\n",
    "            df[col] = df[col].dt.tz_localize('UTC').dt.tz_convert(REGION_TIMEZONE)\n",
    "        except TypeError:\n",
    "            df[col] = df[col].dt.tz_convert(\n",
    "                   'UTC').dt.tz_convert(REGION_TIMEZONE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compress repeat events\n",
    "\n",
    "This function collapses repeated \"is_available\" events and creates two new columns \n",
    "\"available_at\" and \"unavailable_at\", representing a range, where \"available_at\" comes before \"unavailable_at\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collapse_is_available_events(group):\n",
    "    global supply_df\n",
    "    group = group.sort_values(by='event_datetime')\n",
    "\n",
    "    # get time of change of states\n",
    "\n",
    "    # get event_datetime when is_available goes from true to false (becomes unavailable)\n",
    "    # previous event (is_available=True) changed state (is_available=False), indicating becoming unavailable\n",
    "    left = group[(group['is_available'] == False) & (group['is_available'].shift() == True)].rename(\n",
    "        columns={'event_datetime':'unavailable_at'})\n",
    "\n",
    "    # get event_datetime when is_available goes from false to true (becomes available)\n",
    "    # previous event (is_available=False) changed state (is_available=True), indicating becoming available\n",
    "    right = group[(group['is_available'] == True) & (group['is_available'].shift() == False)].rename(\n",
    "        columns={'event_datetime':'available_at'})['available_at'].to_frame()\n",
    "\n",
    "    # can't assume symmetry for events\n",
    "    # can't tell which event comes first\n",
    "    merged_group = pd.merge_asof(left, right, left_on='unavailable_at', right_on='available_at')\n",
    "    supply_df = supply_df.append(merged_group)\n",
    "\n",
    "    global vehicle_df_vin_grouped\n",
    "\n",
    "    if not supply_df.shape[0] % 1000:\n",
    "        print(f'{supply_df.shape[0]} events collapsed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract dow/hour from range of times\n",
    "\n",
    "This function takes a range of datetime indices and extracts dow and hour and places it into a 2d matrix \n",
    "(represented in dataframes by a multi-index)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct large dow/hour df\n",
    "# NOTE: very expensive. should save intermediates so don't have to regenerate\n",
    "def extractor(x):\n",
    "    global mi_df\n",
    "    temp = deepcopy(base_series)\n",
    "    # duration less than 1 hour, does span across slice (hour) ex: [1:30, 2:15]\n",
    "    if x.size == 2 and x[0].hour != x[1].hour:\n",
    "        temp[x[0].day_name(), x[0].hour] += 60 - x[0].minute\n",
    "        temp[x[1].day_name(), x[1].hour] += x[1].minute\n",
    "\n",
    "    # duration less than 1 hour, doesn't span across slice (hour) ex: [1:30, 1:45]\n",
    "    elif x.size == 2 and x[0].hour == x[1].hour:\n",
    "        temp[x[0].day_name(), x[0].hour] += x[1].minute - x[0].minute\n",
    "\n",
    "    # duration greater than 1 hour, does span across slice (hour) ex: [1:30, 2:30, 2:45]\n",
    "    elif x.size == 3 and x[1].hour == x[2].hour:\n",
    "        temp[x[0].day_name(), x[0].hour] += 60 - x[0].minute\n",
    "        temp[x[2].day_name(), x[2].hour] += x[2].minute\n",
    "\n",
    "    else:\n",
    "        # duration greater than 2 hours, ex: [1:30, 2:30, 3:30, 3:45]\n",
    "        # or spans across multiple hours\n",
    "        n = 0\n",
    "        min_marker = x[0].minute\n",
    "        for i, j, k in zip(x.day_name(), x.hour, x.minute):\n",
    "            # each datetimeindex\n",
    "            if n == 0: # first element => 60 - 30 = 30\n",
    "                temp[i, j] += (60 - k)\n",
    "            elif n == (x.size - 1):  # last element, can't assume full hour\n",
    "                if k >= min_marker:\n",
    "                    temp[i, j] += (k - min_marker) # ex: 3:45 - 3:30 = 15m\n",
    "                else:\n",
    "                    temp[i, j] += k  # ex: 3:30 - 3:00 = 30m\n",
    "            elif n == (x.size - 2):  # second to last element, can't assume full hour\n",
    "                temp[i, j] += k  # ex: 3:30 - 3:00 = 30m\n",
    "            else:  # middle of array\n",
    "                temp[i, j] += 60 # ex: 3:30 - 2:30 = 1h\n",
    "            n += 1\n",
    "    mi_df = mi_df.append(temp, ignore_index=True)\n",
    "    # get size incoming vehicle events\n",
    "    global df\n",
    "    # determine size of mi_df\n",
    "    # report every 10000 events\n",
    "    if not mi_df.shape[0] % 10000:\n",
    "        print(f'mask {mi_df.shape[0]/df.shape[0] * 100}% complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Vehicle Event Analysis Intermediates\n",
    "\n",
    "Uses the above functions in order to reduce and extract idle vehicle minutes to be mapped onto a 2d matrix representing dow and hour of the day. Intakes a file and outputs a processed file to be filtered for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file...\n",
      "Done loading file...\n",
      "Took 1234s\n",
      "Converting datetime columns...\n",
      "Done converting datetime columns...\n",
      "Collapsing events...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:6211: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collapse time: 51.43484020233154s\n",
      "Extracting events...\n",
      "mask 24.078109386850944% complete\n",
      "mask 26.48592032553604% complete\n",
      "mask 28.893731264221135% complete\n",
      "mask 31.30154220290623% complete\n",
      "mask 33.70935314159132% complete\n",
      "mask 36.11716408027642% complete\n",
      "mask 38.52497501896151% complete\n",
      "mask 40.93278595764661% complete\n",
      "mask 43.340596896331704% complete\n",
      "mask 45.7484078350168% complete\n",
      "mask 48.15621877370189% complete\n",
      "mask 50.56402971238698% complete\n",
      "mask 52.97184065107208% complete\n",
      "mask 55.379651589757174% complete\n",
      "mask 57.78746252844227% complete\n",
      "mask 60.195273467127365% complete\n",
      "mask 62.60308440581246% complete\n",
      "mask 65.01089534449756% complete\n",
      "mask 67.41870628318264% complete\n",
      "mask 69.82651722186773% complete\n",
      "mask 72.23432816055283% complete\n",
      "mask 74.64213909923792% complete\n",
      "mask 77.04995003792303% complete\n",
      "mask 79.45776097660811% complete\n",
      "mask 81.86557191529322% complete\n",
      "mask 84.2733828539783% complete\n",
      "mask 86.68119379266341% complete\n",
      "mask 89.0890047313485% complete\n",
      "mask 91.4968156700336% complete\n",
      "mask 93.90462660871869% complete\n",
      "mask 96.31243754740377% complete\n",
      "mask 98.72024848608886% complete\n",
      "mask 101.12805942477397% complete\n",
      "mask 103.53587036345905% complete\n",
      "mask 105.94368130214416% complete\n"
     ]
    }
   ],
   "source": [
    "VEHICLE_DT_COLS = ['event_datetime']\n",
    "\n",
    "print('Loading file...')\n",
    "read_init_time = time.time()\n",
    "\n",
    "# get df and clean up\n",
    "vehicle_df = pd.read_csv(\n",
    "    VEHICLE_DATAFILE,\n",
    "    parse_dates=VEHICLE_DT_COLS,\n",
    "    infer_datetime_format=True\n",
    ").dropna()\n",
    "print('Done loading file...')\n",
    "print(f'Took {int(time.time() - read_init_time)}s')\n",
    "\n",
    "print('Converting datetime columns...')\n",
    "convert_datetime_columns(vehicle_df, VEHICLE_DT_COLS)\n",
    "print('Done converting datetime columns...')\n",
    "\n",
    "# group by vin\n",
    "vehicle_df_vin_grouped = vehicle_df.groupby(['vin'])\n",
    "\n",
    "print('Collapsing events...')\n",
    "collapse_init_time = time.time()\n",
    "vehicle_df_vin_grouped.apply(collapse_is_available_events)\n",
    "print(f'Collapse time: {time.time() - collapse_init_time}s')\n",
    "\n",
    "supply_df = supply_df.dropna()\n",
    "supply_df.reset_index(inplace=True)\n",
    "supply_df['idle_duration'] = supply_df['unavailable_at'] - supply_df['available_at']  # duration for analysis\n",
    "supply_df['idle_duration_minutes'] = supply_df['idle_duration'].dt.total_seconds()/60.0\n",
    "\n",
    "# create datetimeindex of periods with the end datetime appended\n",
    "df = supply_df.apply(\n",
    "    lambda x: (pd.date_range(x['available_at'], x['unavailable_at'], freq='H', closed='left')).append(\n",
    "        pd.to_datetime([x['unavailable_at']])), axis=1)\n",
    "\n",
    "print('Extracting events...')\n",
    "extract_init_time = time.time()\n",
    "df.apply(extractor)\n",
    "print(f'Extract time: {time.time() - extract_init_time}s')\n",
    "\n",
    "# merge the big dow/hour mask back with vehicle_update data\n",
    "supply_df = supply_df.merge(mi_df, left_index=True, right_index=True)\n",
    "supply_df.to_csv(f'{VEHICLE_DATAFILE.split(\".\")[0]}_with_dow_hour_mask.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supply_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# App Events Analysis\n",
    "\n",
    "We load the data, clean up timestamps, and add columns representing the following:\n",
    "1. vehicle_nearby: a boolean representing if there was a vehicle within a distance threshold at the time of the app event\n",
    "2. event_hour: extracted hour of app event time in 24H time\n",
    "3. event_dow: extracted day of the week of app event time, as Monday, Tuesday, etc.\n",
    "\n",
    "Change the following parameters of the query to suit your needs:\n",
    "- customer: code name for customer followed by environment with this format: '{customer}-{env}' example: 'darwin-prod'\n",
    "- start date: yyyy-mm-dd, inclusive. example: '2019-05-01'\n",
    "- end date: yyyy-mm-dd, exclusive. example: '2019-05-15'\n",
    "- region: 'data_lake_{region}_prod' where environment represents the region from where this data is pulled. Currently, we only support 'us' and 'eu'.\n",
    "       \n",
    "Events are pulled from Athena (datalake) using this query:\n",
    "```sql\n",
    "select\n",
    "from_iso8601_timestamp(timestamp) AS \"event_datetime\",\n",
    "\"data-distance\",\n",
    "\"data-user_location-lat\",\n",
    "\"data-user_location-lng\",\n",
    "\"data-vehicle_latitude\",\n",
    "\"data-vehicle_longitude\"\n",
    "from data_lake_us_prod.ma_user_activity -- region\n",
    "where tenant_id = 'darwin-prod' -- customer\n",
    "and name = 'DISTANCE_NEAREST_VEHICLE'\n",
    "and timestamp > '2019-05-01' -- start date\n",
    "and timestamp < '2019-05-15'; -- end date\n",
    "```\n",
    "\n",
    "For eiffel, the query changes a bit:\n",
    "\n",
    "```sql\n",
    "select\n",
    "from_iso8601_timestamp(timestamp) AS \"event_datetime\",\n",
    "\"data-closest_vehicle-distance_to_user\",\n",
    "\"data-customer-lat\",\n",
    "\"data-customer-lng\",\n",
    "\"data-closest_vehicle-lat\",\n",
    "\"data-closest_vehicle-lng\"\n",
    "from data_lake_eu_prod.ma_user_activity -- region\n",
    "where name = 'VEHICLE_AVAILABILITY'\n",
    "AND tenant_id = 'eiffel-prod' -- customer\n",
    "and timestamp >= '2019-06-01'\n",
    "and timestamp < '2019-08-01'\n",
    "order by timestamp desc\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# app events constants\n",
    "rental_distance_threshold = 0.25\n",
    "APP_DATA_DATAFILE = 'app_event_data_eiffel_20190601_20190801.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'convert_datetime_columns' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-dd19d9098bc0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     ).dropna()\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mconvert_datetime_columns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapp_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAPP_DATA_DT_COLS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m app_df.columns = ['event_datetime', 'data-distance', 'data-user_location-lat',\n",
      "\u001b[0;31mNameError\u001b[0m: name 'convert_datetime_columns' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "APP_DATA_DT_COLS = ['event_datetime']\n",
    "app_df = pd.read_csv(\n",
    "        APP_DATA_DATAFILE,\n",
    "        parse_dates=APP_DATA_DT_COLS,\n",
    "        infer_datetime_format=True\n",
    "    ).dropna()\n",
    "\n",
    "convert_datetime_columns(app_df, APP_DATA_DT_COLS)\n",
    "\n",
    "app_df.columns = ['event_datetime', 'data-distance', 'data-user_location-lat',\n",
    "                  'data-user_location-lng', 'data-vehicle_latitude', 'data-vehicle_longitude']\n",
    "\n",
    "# create new column to test if within threshold\n",
    "app_df['vehicle_nearby'] = (app_df['data-distance'] * miles_per_meter) < rental_distance_threshold\n",
    "\n",
    "app_df['event_hour'] = app_df['event_datetime'].dt.hour\n",
    "app_df['event_dow'] = app_df['event_datetime'].dt.day_name()\n",
    "\n",
    "app_df.to_csv(f'{APP_DATA_DATAFILE.split(\".\")[0]}_with_threshold_mask.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rental Events Analysis\n",
    "We load the data, clean up timestamps, and add columns representing the following:\n",
    "1. reserved_at_hour: extracted hour of app event time in 24H time\n",
    "2. reserved_at_dow: extracted day of the week of app event time, as Monday, Tuesday, etc.\n",
    "    \n",
    "Change the following parameters of the query to suit your needs:\n",
    "- customer: code name for customer followed by environment with this format: '{customer}-{env}' example: 'darwin-prod'\n",
    "- start date: yyyy-mm-dd, inclusive. example: '2019-05-01'\n",
    "- end date: yyyy-mm-dd, exclusive. example: '2019-05-15'\n",
    "- environment: 'data_lake_{environment}_prod' where environment represents the region from where this data is pulled. Currently, we only support 'us' and 'eu'.\n",
    "    \n",
    "Events are pulled from Athena (datalake) using this query:\n",
    "\n",
    "```sql\n",
    "SELECT \"object_data-rental_id\" as \"rental_id\",\n",
    "          \"object_data-customer_id\" as \"customer_id\",\n",
    "          \"object_data-rental_reserved_at\" as \"reserved_at\",\n",
    "          \"object_data-rental_booked_at\" as \"booked_at\",\n",
    "          \"object_data-rental_ended_at\" as \"ended_at\",\n",
    "          \"object_data-start_location_latitude\" as \"start_location_lat\",\n",
    "          \"object_data-start_location_longitude\" as \"start_location_lng\",\n",
    "          \"object_data-end_location_latitude\" as \"end_location_lat\",\n",
    "          \"object_data-end_location_longitude\" as \"end_location_lng\",\n",
    "          payments.total_to_charge,\n",
    "          payments.credit_amount_used\n",
    " FROM (select distinct \"object_data-rental_id\" as \"rental_id\"\n",
    "       from \"data_lake_us_prod\".\"sa_object_changed\" -- environment\n",
    "       where \"object_data-rental_reserved_at\" > '2019-05-01' -- start date\n",
    "       and \"object_data-rental_reserved_at\" < '2019-05-02' -- end date\n",
    "      ) as rentals_list\n",
    " JOIN \"data_lake_us_prod\".\"sa_object_changed\" -- environment\n",
    " ON \"object_data-rental_id\" = rentals_list.rental_id\n",
    " JOIN (select \"object_data-rental_id\" as \"rental_id\",\n",
    "          \"object_data-total_to_charge\" as \"total_to_charge\",\n",
    "          \"object_data-credit_amount_used\" as \"credit_amount_used\"\n",
    "       from \"data_lake_us_prod\".\"sa_object_changed\" -- environment\n",
    "       where tenant_id = 'darwin-prod' -- customer\n",
    "       and name = 'PAYMENT_LIFECYCLE'\n",
    "       group by \"object_data-rental_id\", \"object_data-total_to_charge\", \"object_data-credit_amount_used\"\n",
    "       having \"object_data-total_to_charge\" is not null\n",
    "      ) as payments \n",
    " ON payments.rental_id = rentals_list.rental_id\n",
    " WHERE tenant_id = 'darwin-prod' -- customer\n",
    "         AND name = 'RENTAL_LIFECYCLE'\n",
    " GROUP BY\n",
    "     \"object_data-rental_id\",\n",
    "     \"object_data-customer_id\",\n",
    "     \"object_data-start_location_latitude\",\n",
    "     \"object_data-start_location_longitude\",\n",
    "     \"object_data-end_location_latitude\",\n",
    "     \"object_data-end_location_longitude\",\n",
    "     \"object_data-rental_reserved_at\",\n",
    "     \"object_data-rental_booked_at\",\n",
    "     \"object_data-rental_ended_at\",\n",
    "     payments.total_to_charge,\n",
    "     payments.credit_amount_used\n",
    " HAVING \"object_data-start_location_latitude\" is NOT null\n",
    "         AND \"object_data-start_location_longitude\" is NOT null\n",
    "         AND \"object_data-end_location_latitude\" is NOT null\n",
    "         AND \"object_data-end_location_longitude\" is NOT null\n",
    "         AND length(split(cast(\"object_data-end_location_latitude\" as varchar), '.')[2]) > 4\n",
    "         AND length(split(cast(\"object_data-end_location_longitude\" as varchar), '.')[2]) > 4\n",
    "         AND length(split(cast(\"object_data-start_location_latitude\" as varchar), '.')[2]) > 4\n",
    "         AND length(split(cast(\"object_data-start_location_longitude\" as varchar), '.')[2]) > 4\n",
    "```\n",
    "\n",
    "for eiffel from db:\n",
    "```sql\n",
    "select \n",
    "  r.id as rental_id,\n",
    "  r.customer_id as customer_id,\n",
    "  r.start_datetime as reserved_at,\n",
    "  r.booked_at as booked_at,\n",
    "  r.end_datetime as ended_at,\n",
    "  loc.lat as start_location_lat,\n",
    "  loc.lng as start_location_lng,\n",
    "  loc.postal_code,\n",
    "  p.total_to_charge as total_to_charge,\n",
    "  p.credit_used_amount as credit_amount_used\n",
    "from carsharing_rental as r\n",
    "  join common_location as loc on r.start_location_id = loc.id\n",
    "  join carsharing_payment on carsharing_payment.rental_id = r.id\n",
    "  join common_payment as p on p.id = carsharing_payment.payment_ptr_id\n",
    "  join common_customer on common_customer.id = r.customer_id\n",
    "  join common_userprofile on common_customer.user_profile_id = common_userprofile.id\n",
    "  join auth_user on common_userprofile.user_id = auth_user.id\n",
    "where auth_user.email not in (select email from common_paymentexemption)\n",
    "  and auth_user.email not like '%ridecell.com'\n",
    "  and auth_user.email not like '%zity%'\n",
    "  and auth_user.email not like '%ferrovial%'\n",
    "  and auth_user.email not like '%renault%'\n",
    "  and r.start_datetime >= '2019-06-01'\n",
    "  and r.start_datetime < '2019-08-01'\n",
    "  and r.cancelled_at is null;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the rentals file\n",
    "RENTAL_DATAFILE = 'rental_data_eiffel_db_20190601_20190801.csv'\n",
    "RENTAL_DT_COLS = ['reserved_at', 'booked_at', 'ended_at']\n",
    "\n",
    "# get df and clean up\n",
    "rental_df = pd.read_csv(\n",
    "    RENTAL_DATAFILE,\n",
    "    parse_dates=RENTAL_DT_COLS,\n",
    "    infer_datetime_format=True\n",
    ").fillna(0)\n",
    "\n",
    "convert_datetime_columns(rental_df, RENTAL_DT_COLS)\n",
    "\n",
    "# extract the rental start dow/hour\n",
    "rental_df['reserved_at_hour'] = rental_df['reserved_at'].dt.hour\n",
    "rental_df['reserved_at_dow'] = rental_df['reserved_at'].dt.day_name()\n",
    "\n",
    "rental_df.to_csv(f'{RENTAL_DATAFILE.split(\".\")[0]}_with_dow_hour_mask.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
